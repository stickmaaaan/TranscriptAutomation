# ğŸ¤– Transkriptor â€” datenschutzfreundliche Transkript-Automatisierung

**Transkriptor** ist eine modulare Streamlit-Anwendung zur lokalen bzw. privaten
Transkription und kontrollierten Anonymisierung von Audioaufnahmen.  
Ziel ist ein praxisfÃ¤higes Werkzeug fÃ¼r z. B. Soziale Arbeit, Forschung oder Dokumentation,
bei dem sensible Daten lokal verbleiben und Ã¼ber eine optionale Sprechererkennung
(â€diarizationâ€œ) strukturiert werden kÃ¶nnen.

---

![screenshot](images/screenshot.png)


## ğŸ” Hauptfunktionen

- Aufnahme Ã¼ber das Mikrofon (Auswahl des EingabegerÃ¤ts).
- Upload von Audiodateien (.wav, .mp3, .m4a).
- Transkription mit `faster-whisper` (lokal, optional GPU-beschleunigt).
- Optionale Vorverarbeitung: Resampling, Normalisierung, einfache Rauschminderung.
- Optionale Anonymisierung (spaCy NER â†’ Ersetzen von PER/LOC/ORG etc.).
- Optionale Sprecher-Diarization (pyannote-Modelle, Hugging Face Token nÃ¶tig fÃ¼r einige Modelle).
- Debug- und Segmentanzeige (ein-/ausschaltbar).
- Fallback-Modus: Wenn kein HF-Token vorhanden, lÃ¤uft die App trotzdem â€” allerdings ohne echte Diarization (Dummy-Speaker).

---


## ğŸ—‚ Projektstruktur
```plaintext
Transkriptor/
â”œâ”€â”€ transcriptor/
â”‚ â”œâ”€â”€ app.py
â”‚ â””â”€â”€  modules/
â”‚ â”‚    â”œâ”€â”€ recorder.py
â”‚ â”‚    â”œâ”€â”€ transcribe.py
â”‚ â”‚    â”œâ”€â”€ speaker_diarization.py
â”‚ â”‚    â”œâ”€â”€ preprocessing.py
â”‚ â”‚    â””â”€â”€ anonymize.py
â”‚ â””â”€â”€ config.json
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```


## ğŸ¤— Hugging Face Token â€” Schritt-fÃ¼r-Schritt (kurzanleitung)
Dieser Schritt ist nur wichtig, wenn die Sprechererkennung genutzt werden soll.

Wenn das nicht gwÃ¼nscht ist, kann man den Schritt hier Ã¼berspringen.



1. Melde dich bei https://huggingface.co an (oder registriere dich).
2. Gehe zu deinem Profil â†’ Settings â†’ Access Tokens (oder: https://huggingface.co/settings/tokens).
3. Erstelle einen neuen Token (New token).
4. WÃ¤hle einen aussagekrÃ¤ftigen Namen.
5. Scope: read (oder repo/read:models) â€” fÃ¼r die meisten AnwendungsfÃ¤lle reicht read.
6. Kopiere den Token und fÃ¼ge ihn in config.json ein.

â„¹ï¸ Zugriff auf pyannote-Modelle / gated Modelle:
Einige pyannote-Modelle sind gated â€” das heiÃŸt: Du musst auf der jeweiligen HF-Modelldetailseite die Bedingungen explizit akzeptieren (Button â€I acceptâ€œ).
Erst nachdem du die Bedingungen akzeptiert hast und einen korrekten HF-Token benutzt, lÃ¤sst sich das Modell per `Pipeline.from_pretrained("pyannote/...", use_auth_token=HF_TOKEN)` laden.
Wenn du keinen Token hast oder die Bedingungen nicht akzeptiert sind, fÃ¤llt das System in den Dummy-Fallback zurÃ¼ck (keine echte Sprechertrennung).

Folgende pyannote-Modelle mÃ¼ssen auf Huggingface akzeptiert werden:

[pyannote/segmentation-3.0](https://huggingface.co/pyannote/segmentation-3.0)

[pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)

[pyannote/speaker-diarization-precision-2](https://huggingface.co/pyannote/speaker-diarization-precision-2)

[pyannote/speaker-diarization](https://huggingface.co/pyannote/speaker-diarization)



## âš™ï¸ Installation (lokal)

### 1. Repo klonen:
```bash
git clone https://github.com/stickmaaaan/TranscriptAutomation.git
cd transcriptor/transcriptor
```

### 2. Virtuelle Umgebung erstellen und aktivieren:
```bash
python -m venv .venv
source .venv/bin/activate   # Linux / macOS
.venv\Scripts\activate      # Windows (PowerShell: .venv\Scripts\Activate.ps1)
```

### 3. AbhÃ¤ngigkeiten installieren:
```bash
pip install -r requirements.txt
```
Hinweis: Manche AbhÃ¤ngigkeiten (z. B. torch) sollten passend zur Hardware (CPU / NVIDIA CUDA / ROCm) installiert werden â€” siehe Abschnitt GPU & KompatibilitÃ¤t weiter unten.


### 4. config.json vorbereiten:
Siehe: Hugging Face Token
```json
{
    "HF_TOKEN": "hf_.......",
}
```

### 5. App starten:
```bash
streamlit run transcriptor/app.py
```


## ğŸ“¦ Requirements
FÃ¼r die Sprechererkennung (pyannote) ist ein Account auf Huggingface.co notwendig!

| Paket            | Zweck                                            |
| ---------------- | ------------------------------------------------ |
| `streamlit`      | Web-UI der Anwendung                             |
| `faster-whisper` | Schnelle Whisper-Transkription auf CPU/GPU       |
| `sounddevice`    | Mikrofonaufnahme                                 |
| `wavio`          | Speichern von WAV-Audio                          |
| `numpy`          | Signalverarbeitung, Audiopuffer                  |
| `scipy`          | Resampling / Preprocessing                       |
| `pydub`          | Formatkonvertierung, Schneiden, Normalisieren    |
| `librosa`        | Audioanalyse (z. B. LautstÃ¤rke, Samplerate)      |
| `noisereduce`    | RauschunterdrÃ¼ckung fÃ¼r Preprocessing            |
| `spacy`          | NLP-Anonymisierung (Namen, Orte, Organisationen) |
| `torch`          | Tensor-Backend / GPU-Beschleunigung              |
| `torchaudio`     | Audio-Backend fÃ¼r Torch                          |
| `pyannote.audio` | **Speaker-Diarization** (Sprechertrennung)       |








## ğŸª² Bekannte Probleme
Da die GPU-Nutzung Ã¼ber CUDA lÃ¤uft, funktioniert die GPU-Nutzung nur mit NVIDIA Grafikkarten!


###   ğŸ§  GPU & KompatibilitÃ¤t (Warum in der Praxis meist NVIDIA/CUDA)

Kurzfassung: Viele ML-Frameworks (PyTorch, TensorRT, CTranslate2, manche model-optimizations) verwenden CUDA, das proprietÃ¤re Compute-Framework von NVIDIA.

NVIDIA GPUs + CUDA die zuverlÃ¤ssigste, mainstream-kompatible Option fÃ¼r GPU-beschleunigte Inferenz.

NVIDIA wird standardmÃ¤ÃŸig unterstÃ¼tzt mit passenden CUDA-Treibern.


AMD GPUs kÃ¶nnen mit ROCm in einigen Setups arbeiten, aber ROCm-UnterstÃ¼tzung ist hardware- und distribution-spezifisch (nur bestimmte AMD-Karten, oft Linux-only).


Intel GPUs (neuere IntegrationslÃ¶sungen): experimental, nicht allgemein unterstÃ¼tzt in vielen Libraries (Stand: StandardeinsatzfÃ¤lle).

Empfohlene Schritte, wenn du GPU nutzen willst:

 - Installiere passende NVIDIA-Treiber + CUDA Toolkit (Version passend zu deiner PyTorch-Version).

```bash
# Beispiel (nicht exakt fÃ¼r jede Version)
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

Es wird dringend Empfohlen GPU-Nutzung zu verwenden!!!

Wenn GPU nicht benutzt wird, dann folgt ein fallback auf CPU-Nutzung â€” App lÃ¤uft weiter, nur langsamer und es kann zu AbstÃ¼rzen fÃ¼hren.




## ğŸ§­ Usage / UI-Bedienung (kurzanleitung)

1. Starte die App: `streamlit run transcriptor/app.py.`

2. WÃ¤hle in der Sidebar:

  - Preprocessing an/aus

  - Anonymizer an/aus

  - Diarization an/aus

  - Force Dummy-Fallback (zum Debug/Test)

3. Nimm eine Aufnahme auf oder lade eine Datei hoch.

4. Klicke Transkription starten â€” wÃ¤hrend der Verarbeitung wird die UI gesperrt. Ergebnis wird angezeigt, danach ist die UI wieder aktiv.

5. Transkript als `.txt` oder `.json` exportieren. 

Debug-Ansichten zeigen Transkript- und Diarization-Segmente (optional).



## ğŸ›  Troubleshooting:

â€Huggingface Token fehltâ€œ
â†’ config.json prÃ¼fen. Wenn kein Token vorhanden ist, fÃ¤llt die App in den Dummy-Fallback (keine echte Sprechererkennung).

â€pyannote-Model konnte nicht geladen werden / Zugriff verweigertâ€œ
â†’ PrÃ¼fe, ob du die Modellseite geÃ¶ffnet und die Bedingungen akzeptiert hast. Stelle sicher, dass der HF_TOKEN die richtige Scope/Privilegien hat.

â€torch.cuda.is_available() == Falseâ€œ
â†’ PrÃ¼fe GPU-Treiber & CUDA-Installation. Alternativ lÃ¤uft alles auf CPU, aber deutlich langsamer.

Audio-Recording Fehler (PortAudio / sounddevice)
â†’ PrÃ¼fe sd.query_devices(), wÃ¤hle richtigen Index, achte auf korrekte Kanalanzahl (mono vs. stereo) und kompatible Sample Rate (16kHZ).

spaCy Modell fehlt (z. B. de_core_news_lg)
â†’ Installieren: python -m spacy download de_core_news_lg oder wÃ¤hle ein kleineres Modell (sm / md) falls Speicher knapp ist.


